# -*- coding: utf-8 -*-
"""CS23M013.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1jFJYSLg4i2M_PWyPNXptyw4vH0hSkI

### Importing required libraries
"""

# !pip install wandb

# from google.colab import drive
# drive.mount('/content/drive')

#required libraries
import matplotlib.pyplot as plt #to plot 
import numpy as np                
import glob
import os
import argparse
import wandb
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets.utils import download_url
from torchvision import datasets
from torch.utils.data import DataLoader, ConcatDataset, random_split


#by default data link
# train_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/'
# test_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/'

# wandb.login(key='5157ae11e5d243722bc57912a56718dc8ef2f734') #Wandb login with my key

"""#Loader"""

def train_load_(train_d ,test_data_directory ,  train__batch_size, test__batch_size, val_Dataset, val__batch_size, boolT=True ):
    # DataLoader for training dataset
    train_batch_load = DataLoader(
        train_d,
        batch_size=train__batch_size,
        shuffle=boolT)  # shuffle the training data if boolT is True

    # DataLoader for testing dataset (assuming 'test_data_directory' is a variable defined elsewhere)
    batch_size = test__batch_size
    test_batch_load = DataLoader(
        test_data_directory,  # assuming test_data_directory is the dataset directory
        batch_size,            # batch size for testing data
        shuffle=boolT)         # shuffle the testing data if boolT is True

    # DataLoader for validation dataset
    val_batch_load = DataLoader(
        val_Dataset,
        batch_size=val__batch_size,
        shuffle=boolT)  # shuffle the validation data if boolT is True
    
    # Return the DataLoader objects for training, validation, and testing datasets
    return train_batch_load, val_batch_load, test_batch_load


"""#Forward"""

def fn(convolutional_layer_1, activation_function_1, maxpooling_layer_1,
       convolutional_layer_2, activation_function_2, maxpooling_layer_2,
       convolutional_layer_3, activation_function_3, maxpooling_layer_3,
       convolutional_layer_4, activation_function_4, maxpooling_layer_4,
       convolutional_layer_5, activation_function_5, maxpooling_layer_5,
       fully_connected_layer_1, dropout_layer_first,
       fully_connected_layer_2, dropout_layer_second, _softmax, data_x):
    
    # Layer 1: Convolutional, Activation, and Maxpooling
    data_x = convolutional_layer_1(data_x)  # Apply convolutional layer 1
    shape_1 = data_x.shape[1]  # Get shape for later use
    data_x = activation_function_1(data_x)  # Apply activation function 1
    data_x = maxpooling_layer_1(data_x)  # Apply maxpooling layer 1

    # Layer 2: Convolutional, Activation, and Maxpooling
    data_x = convolutional_layer_2(data_x)  # Apply convolutional layer 2
    shape_1 = data_x.shape[1]  # Get shape for later use
    data_x = activation_function_2(data_x)  # Apply activation function 2
    data_x = maxpooling_layer_2(data_x)  # Apply maxpooling layer 2

    # Layer 3: Convolutional, Activation, and Maxpooling
    data_x = convolutional_layer_3(data_x)  # Apply convolutional layer 3
    shape_1 = data_x.shape[1]  # Get shape for later use
    data_x = activation_function_3(data_x)  # Apply activation function 3
    data_x = maxpooling_layer_3(data_x)  # Apply maxpooling layer 3

    # Layer 4: Convolutional, Activation, and Maxpooling
    data_x = convolutional_layer_4(data_x)  # Apply convolutional layer 4
    shape_1 = data_x.shape[1]  # Get shape for later use
    data_x = activation_function_4(data_x)  # Apply activation function 4
    data_x = maxpooling_layer_4(data_x)  # Apply maxpooling layer 4

    # Layer 5: Convolutional, Activation, and Maxpooling
    data_x = convolutional_layer_5(data_x)  # Apply convolutional layer 5
    data_x = activation_function_5(data_x)  # Apply activation function 5
    shape_0 = data_x.shape[1]  # Get shape for later use
    data_x = maxpooling_layer_5(data_x)  # Apply maxpooling layer 5

    # Reshape for Fully Connected Layers
    shape_1 = data_x.shape[1]  # Get shape dimension 1
    shape_2 = data_x.shape[2]  # Get shape dimension 2
    shape_3 = data_x.shape[3]  # Get shape dimension 3

    shape_all = shape_1 * shape_2 * shape_3  # Calculate total shape
    data_x = data_x.view(-1, shape_all)  # Reshape data_x

    # Fully Connected Layer 1 and Dropout
    data_x = fully_connected_layer_1(data_x)  # Apply fully connected layer 1
    data_x = dropout_layer_first(data_x)  # Apply dropout layer after fully connected 1

    # Fully Connected Layer 2 and Dropout
    data_x = fully_connected_layer_2(data_x)  # Apply fully connected layer 2
    data_x = dropout_layer_second(data_x)  # Apply dropout layer after fully connected 2

    # Output Layer with Softmax
    output = _softmax(data_x)  # Apply softmax to get the final output

    return data_x

"""#Transform Data
"""
def transform_datasets_(train_data_directory, test_data_directory, train_batch_size, test_batch_size, val_batch_size, val_Dataset, train_d, image_size, mean, std, degree):
    # Define a series of transformations to be applied to the training dataset
    transformer2 = transforms.Compose([
        transforms.Resize(image_size),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomVerticalFlip(0.02),
        transforms.RandomRotation(degree),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    # Apply the defined transformations to create an augmented dataset
    augmented_dataset = torchvision.datasets.ImageFolder(root=train_data_directory, transform=transformer2)

    # Calculate the size of the augmented dataset to be used (20% of the original dataset)
    aug_d = len(augmented_dataset)
    aug_d = 0.2 * aug_d
    augmented_dataset_size = int(aug_d)

    # Randomly split the augmented dataset into a subset of the desired size
    augmented_dataset, _ = random_split(augmented_dataset, [augmented_dataset_size, len(augmented_dataset) - augmented_dataset_size])

    # Concatenate the augmented dataset with the original training dataset
    train__batch_size = train_batch_size
    train_d = ConcatDataset([train_d, augmented_dataset])

    # Define batch sizes for training, testing, and validation
    test__batch_size = test_batch_size
    val__batch_size = val_batch_size

    # Return the configured dataloaders using the modified training dataset
    return train_load_(train_d,  test_data_directory , train__batch_size, test__batch_size, val_Dataset, val__batch_size, boolT=True )



"""## Loading the iNatuaralist Dataset"""

def data_fetch(apply_data_augmentation, train_data_directory, test_data_directory, train_batch_size, val_batch_size, test_batch_size, image_size, mean, std):

    # Define the transformation pipeline for data preprocessing
    trans_model_1 = transforms.Compose([
        transforms.Resize(image_size),  # Resize images to specified image_size
        transforms.ToTensor(),           # Convert images to tensors
        transforms.Normalize(mean, std)  # Normalize images with provided mean and std
    ])

    # Load the training dataset with the defined transformations
    train_d = torchvision.datasets.ImageFolder(root=train_data_directory, transform=trans_model_1)

    # Calculate the size of the training dataset
    temp = len(train_d)

    # Split the training dataset into training and validation sets (80% training, 20% validation)
    temp = 0.8 * temp
    degree = 45
    train_d, val_Dataset = random_split(train_d, [int(temp), len(train_d) - int(temp)])

    # Check if data augmentation is to be applied
    if apply_data_augmentation == True:
        # Call function to transform datasets with additional augmentation
        return transform_datasets_(test_data_directory, train_data_directory, train_batch_size, test_batch_size, val_batch_size, val_Dataset, train_d, image_size, mean, std, degree)

    # If no data augmentation, proceed with regular data loading
    return train_load_(train_d, train_batch_size, test_batch_size, val_Dataset, val_batch_size, boolT=True)



"""
## Setting device to cuda if available
"""
#check device if GPU or CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

"""# Create a CNN Model"""
class initialize_lenet_CNN(torch.nn.Module):
    def __init__(self, number_of_classes, size_of_kernel, num_of_filters, activation_function, filter_multiplier, dropout_factor):
        super(initialize_lenet_CNN, self).__init__()

        # Mapping of activation function names to PyTorch activation modules
        activation_map = {
            'ReLU': torch.nn.ReLU(),
            'SiLU': torch.nn.SiLU(),
            'Mish': torch.nn.Mish(),
            'GELU': torch.nn.GELU()
        }

        # Layer 1: Convolution, Activation, MaxPooling
        self.convolutional_layer_1 = torch.nn.Conv2d(in_channels=3, out_channels=num_of_filters, kernel_size=size_of_kernel)
        prev_number_filters = num_of_filters
        self.size = (256 - size_of_kernel)  # Compute output size after convolution
        self.activation_function_1 = activation_map.get(activation_function, torch.nn.ReLU())
        self.maxpooling_layer_1 = torch.nn.MaxPool2d(kernel_size=size_of_kernel, stride=2)
        self.size = self.size // 2  # Reduce size due to max pooling
        number_filters = int(num_of_filters * filter_multiplier)

        # Layer 2: Convolution, Activation, MaxPooling
        self.convolutional_layer_2 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=size_of_kernel)
        self.size = (self.size - size_of_kernel)  # Compute output size after convolution
        prev_number_filters = number_filters
        number_filters = int(number_filters * filter_multiplier)
        self.activation_function_2 = activation_map.get(activation_function, torch.nn.ReLU())
        self.maxpooling_layer_2 = torch.nn.MaxPool2d(kernel_size=size_of_kernel, stride=2)
        self.size = self.size // 2  # Reduce size due to max pooling

        # Layer 3: Convolution, Activation, MaxPooling
        self.convolutional_layer_3 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=size_of_kernel)
        self.size = (self.size - size_of_kernel)  # Compute output size after convolution
        prev_number_filters = number_filters
        number_filters = int(number_filters * filter_multiplier)
        self.activation_function_3 = activation_map.get(activation_function, torch.nn.ReLU())
        self.maxpooling_layer_3 = torch.nn.MaxPool2d(kernel_size=size_of_kernel, stride=2)
        self.size = self.size // 2  # Final output size before fully connected layers (50 x 16 x 16)

        # Layer 4: Convolution, Activation, MaxPooling
        self.convolutional_layer_4 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=size_of_kernel)
        self.size = (self.size - size_of_kernel)  # Compute output size after convolution
        prev_number_filters = number_filters
        number_filters = int(number_filters * filter_multiplier)
        self.activation_function_4 = activation_map.get(activation_function, torch.nn.ReLU())
        self.maxpooling_layer_4 = torch.nn.MaxPool2d(kernel_size=size_of_kernel, stride=2)
        self.size = self.size // 2  # Reduce size due to max pooling

        # Layer 5: Convolution, Activation, MaxPooling
        self.convolutional_layer_5 = torch.nn.Conv2d(in_channels=prev_number_filters, out_channels=number_filters, kernel_size=size_of_kernel)
        self.size = (self.size - size_of_kernel)  # Compute output size after convolution
        prev_number_filters = number_filters
        stride_size = 2
        self.activation_function_5 = activation_map.get(activation_function, torch.nn.ReLU())
        self.maxpooling_layer_5 = torch.nn.MaxPool2d(kernel_size=size_of_kernel, stride=stride_size)
        self.size = self.size // stride_size  # Final output size before fully connected layers (50 x 2 x 2)

        # Calculate in_features for fully connected layers
        self.size = self.size * self.size * prev_number_filters  # Flatten feature map
        size_ = self.size

        # Fully Connected Layer 1: Linear, Dropout
        self.fully_connected_layer_1 = torch.nn.Linear(in_features=size_, out_features=size_)
        self.dropout_layer_first = torch.nn.Dropout(dropout_factor)

        # Fully Connected Layer 2: Linear (Output layer), Dropout, Softmax
        self.fully_connected_layer_2 = torch.nn.Linear(in_features=size_, out_features=number_of_classes)
        dimension_ = 1
        self.dropout_layer_second = torch.nn.Dropout(dropout_factor)
        self._softmax = torch.nn.LogSoftmax(dim=dimension_)

    def forward(self, data_x):
        # Forward pass with all defined layers
        return fn(self.convolutional_layer_1, self.activation_function_1, self.maxpooling_layer_1,
                  self.convolutional_layer_2, self.activation_function_2, self.maxpooling_layer_2,
                  self.convolutional_layer_3, self.activation_function_3, self.maxpooling_layer_3,
                  self.convolutional_layer_4, self.activation_function_4, self.maxpooling_layer_4,
                  self.convolutional_layer_5, self.activation_function_5, self.maxpooling_layer_5,
                  self.fully_connected_layer_1, self.dropout_layer_first,
                  self.fully_connected_layer_2, self.dropout_layer_second,
                  self._softmax, data_x)

"""# Training the model"""
def train(train_data_directory, test_data_directory , convolutional_neural_network, learning_rate, epochs, train_batch_load, val_batch_load, apply_wandb_log):
    # Define the loss function for classification
    loss_function = torch.nn.CrossEntropyLoss()
    
    epoch = 0
    # Define the Adam optimizer with specified learning rate and weight decay
    optimizer = torch.optim.Adam(params=convolutional_neural_network.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    while epoch < epochs:
        acc_train = 0
        loss_train = 0
        
        # Set the model to training mode
        convolutional_neural_network.train()
        
        # Iterate through the training dataset batches
        for i, (images, labels) in enumerate(train_batch_load):
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()  # Reset gradients
            predicted_y = convolutional_neural_network(images)  # Forward pass
            loss = loss_function(predicted_y, labels)  # Calculate loss
            loss.backward()  # Backpropagation
            optimizer.step()  # Update weights

            loss_train += loss.item()  # Accumulate training loss

            _, prediction = torch.max(predicted_y.data, 1)
            acc_train += int(torch.sum(prediction == labels.data))  # Calculate training accuracy
        
        # Calculate average training accuracy and loss
        acc_train = acc_train / len(glob.glob(train_data_directory + '/**/*.jpg'))
        loss_train = loss_train / len(glob.glob(train_data_directory + '/**/*.jpg'))
        
        # Print training accuracy and loss for the epoch
        print(f"Epochs : {epoch+1} Train Accuracy : {acc_train} Train Loss {loss_train}")

        # Validation
        with torch.no_grad():
            test_accuracy = 0
            test_loss = 0
            convolutional_neural_network.eval()  # Set the model to evaluation mode
            
            # Iterate through the validation dataset batches
            for i, (images, labels) in enumerate(val_batch_load):
                images = images.to(device)
                labels = labels.to(device)
                predicted_y = convolutional_neural_network(images)

                # Calculate validation loss
                loss = loss_function(predicted_y, labels)
                test_loss += loss.item()

                _, predicted = torch.max(predicted_y.data, 1)
                test_accuracy += int(torch.sum(predicted == labels.data))  # Calculate validation accuracy
            
            # Calculate average validation accuracy and loss
            test_accuracy = test_accuracy / len(glob.glob(test_data_directory + '/**/*.jpg'))
            test_loss = test_loss / len(glob.glob(test_data_directory + '/**/*.jpg'))
            
            # Print validation accuracy and loss for the epoch
            print(f"Epochs : {epoch+1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}")
            
            # Log metrics using Weights & Biases if specified
            if apply_wandb_log:
                wandb.log({"train_accuracy": acc_train, "train_loss": loss_train, "val_accuracy": test_accuracy, "val_error": test_loss})

        epoch += 1  # Increment epoch counter


"""Main function"""
def main(train_data_directory, test_data_directory, apply_data_augmentation, number_of_classes, kernel_size, train_batch_size, val_batch_size, test_batch_size, num_of_filters, activation_function, filter_multiplier, learning_rate, epochs, apply_wandb_log, dropout_factor, image_size):
    # Mean and standard deviation for normalization
    mean = [0.4602, 0.4495, 0.3800]
    std = [0.2040, 0.1984, 0.1921]

    # Print start message
    print("start=>")

    # Load data batches using data_fetch function
    train_batch_load, val_batch_load, test_batch_load = data_fetch(apply_data_augmentation, train_data_directory, test_data_directory, train_batch_size, val_batch_size, test_batch_size, image_size, mean, std)

    # Initialize LeNet-like CNN model
    convolutional_neural_network = initialize_lenet_CNN(number_of_classes, kernel_size, num_of_filters, activation_function, filter_multiplier, dropout_factor)

    # Move model to appropriate device (assuming 'device' is defined elsewhere)
    convolutional_neural_network = convolutional_neural_network.to(device)

    # Train the model
    train(train_data_directory, test_data_directory , convolutional_neural_network, learning_rate, epochs, train_batch_load, val_batch_load, apply_wandb_log)

    # Print completion message
    print("Done!!")

    # Return the trained model
    return convolutional_neural_network

"""Configuration"""
# Configuration dictionary for a hyperparameter tuning experiment

config = {
    'name': 'cs23m013',  # Unique identifier for this experiment
    'metric': {
        'goal': 'maximize',  # Goal of the optimization (maximize val_accuracy)
        'name': 'val_accuracy'  # Metric to optimize
    },
    "method": "random",  # Using random search for hyperparameter optimization
    "project": "Deep_Learning_A2",  # Project name or identifier

    "parameters": {
        # Hyperparameters to tune

        "image_size": {
            "values": [224, 256]  # Possible values for image size
        },

        "epochs": {
            "values": [5, 10]  # Possible values for number of epochs
        },

        "activation_function": {
            "values": ["LeakyReLU", "Mish", "SiLU", "GELU", "ReLU"]  # Activation function choices
        },

        "dropout_factor": {
            "values": [0, 0.1, 0.2, 0.3, 0.4]  # Dropout rate choices
        },

        "num_of_filters": {
            "values": [32, 64]  # Number of filters in convolutional layers
        },

        "filter_multiplier": {
            "values": [0.3, 0.4, 0.5]  # Multiplier for filter sizes
        },

        "learning_rate": {
            "values": [0.0001, 0.0003]  # Learning rate choices
        },

        "apply_data_augmentation": {
            "values": [True]  # Whether to apply data augmentation (boolean choice)
        },
    }
}




"""Train"""
if __name__ == "__main__":
    # Creating an ArgumentParser object with a specified formatter class
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    # Description of the parser and adding arguments for hyperparameters
    parser = argparse.ArgumentParser(description="Stores all the hyperparameters for the model.")
    
    # Adding arguments with default values and help messages
    parser.add_argument("-wp", '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard', type=str, default='Deep_Learning_A2')
    parser.add_argument("-we", "--wandb_entity", type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="cs23m013")
    parser.add_argument("-lg", "--logger", type=bool, default=False, choices=[True, False], help="Log to wandb or not")
    parser.add_argument('-lr', '--learning_rate', help='Learning rate used to optimize model parameters', type=float, default=0.001)
    parser.add_argument("-tsbs", "--test_batch_size", type=int, default=16)
    parser.add_argument('-ac', '--activation', help='choices: ["LeakyReLU", "Mish", "SiLU", "GELU", "ReLU"]', choices=["LeakyReLU", "Mish", "SiLU", "GELU", "ReLU"], type=str, default='ReLU')
    parser.add_argument("-df", "--dropout_factor", help="Dropout factor", default=0.3, type=float)
    parser.add_argument("-ff", "--filter_multiplier", default="1", type=float, choices=[1, 0.5, 2])
    parser.add_argument("-nc", "--number_of_classes", type=int, default=10)
    parser.add_argument("-ks", "--kernel_size", type=int, default=3)
    parser.add_argument("-nf", "--num_filters", default="3", type=int, help="Number of filters in the convolutional neural network.")
    parser.add_argument("-trbs", "--train_batch_size", type=int, default=64)
    parser.add_argument("-vbs", "--val_batch_size", type=int, default=16)
    parser.add_argument("-imgs", "--image_size", type=int, default=256, choices=[246, 256])
    parser.add_argument("-aug", "--apply_data_augmentation", type=bool, default=True, choices=[True, False])
    parser.add_argument('-ep', '--epochs', help="Number of epochs to train neural network.", type=int, default=3)
    parser.add_argument('-trd', '--train_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/')
    parser.add_argument('-tsd', '--test_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/')

    # Parsing the arguments from command line
    args = parser.parse_args()
    
   

    # Creating a sweep configuration for Weights & Biases
    sweep_id = wandb.sweep(config, project=args.wandb_project)
    print('sweep_id', sweep_id)
    
    # Initializing Wandb if logger is enabled
    if args.logger:
        wandb.init(config=args, project=args.wandb_project, entity=args.wandb_entity, reinit='true')

    # Printing selected arguments
    print("Test Dataset Path:", args.test_data_directory)
    print("WandbProject:", args.wandb_project)
    print("Wandb Entity:", args.wandb_entity)
    print("Activation:", args.activation)
    print("Epochs:", args.epochs)
    print("Number of filter:", args.num_filters)
    print("Learning rate:", args.learning_rate)
    print("Dropout factor:", args.dropout_factor)
    print("Num factor:", args.filter_multiplier)

    # Calling the main function with parsed arguments
    main(args.train_data_directory, args.test_data_directory, args.apply_data_augmentation, args.number_of_classes,
         args.kernel_size, args.train_batch_size, args.val_batch_size, args.test_batch_size, args.num_filters,
         args.activation, args.filter_multiplier, args.learning_rate, args.epochs, args.logger, args.dropout_factor,
         (256, 256))
    
    # Finishing Weights & Biases run
    wandb.finish()




