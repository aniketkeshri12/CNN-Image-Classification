# -*- coding: utf-8 -*-
"""CS23M013.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGQK206leMukr8SoYrLQFkBSQxWOE

#Importing libraries
"""

import glob
import torch
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import argparse

# !pip install wandb

# from google.colab import drive
# drive.mount('/content/drive')

import wandb
from torchvision.datasets import FashionMNIST
from torchvision import datasets, models
from torchvision.datasets.utils import download_url
from torch.utils.data import DataLoader, ConcatDataset, random_split

# wandb.login(key='5157ae11e5d243722bc57912a56718dc8ef2f734') #Wandb login with my key

"""#Dataset Link"""
#adding default data links
# train_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/'
# test_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/'

"""### Importing the pretrained VGG Net16 model"""

# Load the VGG-16 model with pre-trained weights
vggnet = torchvision.models.vgg16(weights=True)

# Display the VGG-16 model
vggnet


"""### Setting device to 'cuda' if GPU is available."""
#check if GPU active or not
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device


wandb.login(key='5157ae11e5d243722bc57912a56718dc8ef2f734') #Wandb login with my key
"""#Layer Add"""

def add_layer(in_features, out_feature, dim):
    # Update the 6th layer of the vggnet classifier with a new Linear layer
    vggnet.classifier[6] = torch.nn.Linear(in_features, out_feature)
    
    # Add a new module (layer) at index 7 in the classifier with LogSoftmax activation
    vggnet.classifier.add_module("7", torch.nn.LogSoftmax(dim))
    
    # Note: vggnet is not being modified here; it's assumed to be a pre-existing object.
    # Move the vggnet model to the specified device (e.g., GPU) for computation
    vggnet.to(device)


"""#Augment train data"""

def augment_train_data(image_size, degree, mean, std, train_d , train_data_directory):
    # Define a series of transformations to apply to the images
    transformer2 = transforms.Compose([
        transforms.Resize(image_size),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomVerticalFlip(0.02),
        transforms.RandomRotation(degree),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    
    # Create an augmented dataset using ImageFolder with the specified transformations
    augmented_dataset = torchvision.datasets.ImageFolder(train_data_directory, transform=transformer2)
    
    # Calculate the size of the subset to keep after augmentation (20% of the augmented dataset)
    aug_d = len(augmented_dataset)
    aug_d = 0.2 * aug_d
    
    # Randomly split the augmented dataset to retain only a portion (20%)
    augmented_dataset, _ = random_split(augmented_dataset, [int(aug_d), len(augmented_dataset) - int(aug_d)])
    
    # Concatenate the augmented dataset with the original training dataset
    train_d = ConcatDataset([train_d, augmented_dataset])



"""#Data Loader"""
def data_loader(train_d, train_batch_size, is_shuffle, test_batch_size, val_Dataset, val_batch_size , test_data_directory):
    # Create DataLoader for training data
    train_batch_load = DataLoader(
        train_d,
        train_batch_size,
        is_shuffle)
    
    # Create DataLoader for test data (assuming test_data_directory is a variable)
    test_batch_load = DataLoader(
        test_data_directory,  # Assuming this variable contains the path to test data
        test_batch_size,
        is_shuffle)
    
    # Create DataLoader for validation data
    val_batch_load = DataLoader(
        val_Dataset,
        val_batch_size,
        is_shuffle)
    
    # Return the DataLoader objects for training, validation, and test data
    return train_batch_load, val_batch_load, test_batch_load


"""# Loading the iNaturalist Dataset"""

def load_dataset(train_batch_size, val_batch_size, test_batch_size, image_size, apply_data_augmentation, degree, mean, std , train_data_directory , test_data_directory):
    # Define transformation pipeline for dataset preprocessing
    transformer1 = transforms.Compose([
        transforms.Resize(image_size),  # Resize image to specified size
        transforms.ToTensor(),           # Convert image to tensor
        transforms.Normalize(mean, std)  # Normalize image with given mean and std
    ])

    # Load training dataset from directory with defined transformations
    train_d = torchvision.datasets.ImageFolder(train_data_directory, transform=transformer1)
    temp = len(train_d)
    temp = 0.8 * temp

    # Split training dataset into training and validation subsets
    train_d, val_Dataset = random_split(train_d, [int(temp), len(train_d) - int(temp)])

    # Apply data augmentation if specified
    if (apply_data_augmentation):
        augment_train_data(image_size, degree, mean, std, train_d , train_data_directory)

    # Return data loaders for training and validation datasets
    return data_loader(train_d, train_batch_size, True, test_batch_size, val_Dataset, val_batch_size , test_data_directory)


"""### Freezing the Model Parameters"""

# Loop through each parameter in the VGGNet model.
for param in vggnet.parameters():
    boolT=False  # Assign False to boolT variable (Note: This line seems unnecessary and doesn't affect the rest of the code.)
    param.requires_grad = boolT  # Set the requires_grad attribute of the parameter to the value of boolT (which is False).


"""### Adding One more layer to the model."""
#call layer function to add extra layer
add_layer(in_features=4096 , out_feature=10 , dim=1)

"""## Train"""

def main(model, learning_rate, epochs, apply_wandb_log, image_size, apply_data_augmentation, train_data_directory, test_data_directory):
    # Mean and standard deviation for normalization
    mean = [0.4602, 0.4495, 0.3800]
    std = [0.2040, 0.1984, 0.1921]
    # Degree for data augmentation rotation
    degree = 45
    
    # Load train, validation, and test datasets
    train_batch_load, val_batch_load, test_batch_load = load_dataset(64, 16, 16, image_size, apply_data_augmentation, degree, mean, std , train_data_directory , test_data_directory)
    
    # Define loss function (CrossEntropyLoss) and optimizer (Adam)
    loss_function = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    epoch = 0
    # Loop over epochs
    while epoch < epochs:
        train_accuracy = 0
        train_loss = 0
        test_accuracy = 0
        test_loss = 0
        
        # Set model to train mode
        model.train()
        
        # Iterate through batches in the training dataset
        for i, (images, labels) in enumerate(train_batch_load):
            images = images.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()  # Zero gradients
            y_pred = model(images)  # Forward pass
            
            loss = loss_function(y_pred, labels)  # Calculate loss
            loss.backward()  # Backward pass
            optimizer.step()  # Update model parameters
            
            train_loss += loss.item()  # Accumulate training loss
            
            _, prediction = torch.max(y_pred.data, 1)
            train_accuracy += int(torch.sum(prediction == labels.data))  # Calculate training accuracy
        
        # Calculate overall training accuracy and loss for the epoch
        count_train = len(glob.glob(train_data_directory + '/**/*.jpg'))
        train_accuracy = train_accuracy / count_train
        train_loss = train_loss / count_train
        print(f"Epochs : {epoch + 1} Train Accuracy : {train_accuracy / count_train} Train Loss {train_loss}")
        
        # Evaluate the model on the validation dataset
        with torch.no_grad():
            model.eval()  # Set model to evaluation mode
            
            for i, (images, labels) in enumerate(val_batch_load):
                images = images.to(device)
                labels = labels.to(device)
                
                y_pred = model(images)
                
                loss = loss_function(y_pred, labels)
                test_loss += loss.item()  # Accumulate validation loss
                
                _, predicted = torch.max(y_pred.data, 1)
                test_accuracy += int(torch.sum(predicted == labels.data))  # Calculate validation accuracy
            
            # Calculate overall validation accuracy and loss for the epoch
            count_test = len(glob.glob(test_data_directory + '/**/*.jpg'))
            test_accuracy = test_accuracy / count_test
            test_loss = test_loss / count_test
            
            print(f"Epochs : {epoch + 1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}")
            
            # Log metrics to wandb (if enabled)
            if apply_wandb_log:
                wandb.log({"train_accuracy": train_accuracy, "train_loss": train_loss, "val_accuracy": test_accuracy, "val_error": test_loss})
        
        epoch += 1


"""## Train"""

if __name__ == "__main__":
    # Initialize argument parser
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    # Define arguments for hyperparameters
    parser.add_argument("-wp", '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard', type=str, default='Deep_Learning_A2')
    parser.add_argument("-we", "--wandb_entity", type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="cs23m013")
    parser.add_argument('-ep', '--epochs', help="Number of epochs to train neural network.", type=int, default=10)
    parser.add_argument("-nf", "--num_filters", default="3", type=int, help="Number of filters in the convolutional neural network.")
    parser.add_argument('-lr', '--learning_rate', help='Learning rate used to optimize model parameters', type=float, default=0.001)
    parser.add_argument('-ac', '--activation', help='choices: ["Mish", "SiLU", "GELU", "ReLU"]', choices=["Mish", "SiLU", "GELU", "ReLU"], type=str, default='ReLU')
    parser.add_argument("-ff", "--filter_multiplier", default="1", type=float, choices=[1, 0.5, 2])
    parser.add_argument("-df", "--dropout_factor", help="Dropout factor", default=0.3, type=float)
    parser.add_argument("-nc", "--number_of_classes", type=int, default=10)
    parser.add_argument("-ks", "--kernel_size", type=int, default=3)
    parser.add_argument("-lg", "--logger", type=bool, default=False, choices=[True, False], help="Log to wandb or not")
    parser.add_argument("-ag", "--augment", default=True, help="Do Augmentation", type=bool, choices=[True, False])
    parser.add_argument("-is", "--imgSize", default=256, help="Image Size", type=int, choices=[246, 256])
    parser.add_argument('-trd', '--train_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/')
    parser.add_argument('-tsd', '--test_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/')

    # Parse arguments from command line
    args = parser.parse_args()

    # Display parsed arguments
    print("Test Dataset Path:", args.test_data_directory)
    print("Wandb Project:", args.wandb_project)
    print("Wandb Entity:", args.wandb_entity)
    print("Activation:", args.activation)
    print("Epochs:", args.epochs)
    print("Number of filter:", args.num_filters)
    print("Learning rate:", args.learning_rate)
    print("Dropout factor:", args.dropout_factor)
    print("Num factor:", args.filter_multiplier)

    # Modify the model's final layer and prepare for training
    vggnet.classifier[6] = torch.nn.Linear(4096, 10)
    vggnet.classifier.add_module("7", torch.nn.LogSoftmax(dim=1))
    vggnet.to(device)

    # Set image size based on arguments
    image_size = (args.imgSize, args.imgSize)

    # Call main function with specified arguments
    main(vggnet, args.learning_rate, args.epochs, args.logger, image_size, args.augment, args.train_data_directory, args.test_data_directory)



