# -*- coding: utf-8 -*-
"""CS23M013.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGQK206leMukr8SoYrLQFkBSQxWOE

#Importing libraries
"""

import glob
import torch
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import argparse

# !pip install wandb

# from google.colab import drive
# drive.mount('/content/drive')

import wandb
from torchvision.datasets import FashionMNIST
from torchvision import datasets, models
from torchvision.datasets.utils import download_url
from torch.utils.data import DataLoader, ConcatDataset, random_split

wandb.login(key='5157ae11e5d243722bc57912a56718dc8ef2f734') #Wandb login with my key

"""#Dataset Link"""

train_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/'
test_data_directory = '/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/'

"""### Importing the pretrained VGG Net16 model"""

vggnet = torchvision.models.vgg16(weights=True)
vggnet

"""### Setting device to 'cuda' if GPU is available."""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

"""#Layer Add"""

def add_layer(in_features , out_feature , dim):
    vggnet.classifier[6] = torch.nn.Linear( in_features , out_feature)
    vggnet.classifier.add_module("7", torch.nn.LogSoftmax(dim))
    vggnet
    vggnet.to(device)

"""#Augment train data"""

def augment_train_data(image_size , degree , mean , std , train_d):
        transformer2 = transforms.Compose([
        transforms.Resize(image_size),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomVerticalFlip(0.02),
        transforms.RandomRotation(degree),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
        ])
        augmented_dataset = torchvision.datasets.ImageFolder(train_data_directory, transform=transformer2)
        aug_d= len(augmented_dataset)
        aug_d= 0.2 *aug_d
        augmented_dataset, _  =  random_split(augmented_dataset, [int(aug_d), len(augmented_dataset) - int(aug_d)])
        train_d = ConcatDataset([train_d, augmented_dataset])

"""#Data Loader"""

def data_loader(train_d  ,train_batch_size , is_shuffle , test_batch_size ,  val_Dataset , val_batch_size):
    train_batch_load = DataLoader(
        train_d,
        train_batch_size,
        is_shuffle)
    test_batch_load = DataLoader(
        test_data_directory,
        test_batch_size,
        is_shuffle)
    val_batch_load = DataLoader(
        val_Dataset,
        val_batch_size,
        is_shuffle)
    return train_batch_load, val_batch_load, test_batch_load

"""# Loading the iNaturalist Dataset"""

def load_dataset(train_batch_size, val_batch_size, test_batch_size , image_size , apply_data_augmentation ,degree , mean, std):
    transformer1 = transforms.Compose([
        transforms.Resize(image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    train_d = torchvision.datasets.ImageFolder(train_data_directory, transform=transformer1)
    temp =len(train_d)
    temp = 0.8* temp

    train_d, val_Dataset = random_split(train_d, [int(temp), len(train_d) - int(temp)])

    if (apply_data_augmentation):
        augment_train_data(image_size , degree , mean , std , train_d)
    return data_loader(train_d  ,train_batch_size , True , test_batch_size ,  val_Dataset , val_batch_size)

"""### Freezing the Model Parameters"""

for param in vggnet.parameters():
    boolT=False
    param.requires_grad = boolT

"""### Adding One more layer to the model."""

add_layer(in_features=4096 , out_feature=10 , dim=1)

"""## Train"""

def main(model, learning_rate, epochs, apply_wandb_log , image_size , apply_data_augmentation , train_data_directory , test_data_directory):
    mean=[0.4602, 0.4495, 0.3800]
    degree=45
    std=[0.2040, 0.1984, 0.1921]
    train_batch_load, val_batch_load, test_batch_load = load_dataset(64, 16, 16 , image_size ,apply_data_augmentation ,degree, mean , std)
    loss_function = torch.nn.CrossEntropyLoss()
    epoch=0
    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay =1e-4)
    while epoch < epochs:
        train_accuracy = 0
        train_loss = 0
        test_accuracy = 0
        test_loss = 0
        model.train()
        for i, (images, labels) in enumerate(train_batch_load):

            images= images.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()   # doing zero gradient.
            y_pred = model(images) #forward Propagation
            loss = loss_function(y_pred, labels) # Calculating Loss.
            loss.backward() # Backward Propagation
            optimizer.step() # update rule

            train_loss =train_loss +  loss.item()

            _, prediction = torch.max(y_pred.data, 1)
            train_accuracy =train_accuracy +  int(torch.sum(prediction == labels.data))
        count_train = len(glob.glob(train_data_directory+'/**/*.jpg'))
        train_accuracy = train_accuracy / count_train
        train_loss = train_loss / count_train
        print(f"Epochs : {epoch+1} Train Accuracy : {train_accuracy / count_train} Train Loss {train_loss}")


        with torch.no_grad():
            model.eval()
            for i, (images, labels) in enumerate(val_batch_load):
                images = images.to(device)
                labels = labels.to(device)

                y_pred = model(images)

                loss = loss_function(y_pred, labels)
                test_loss = test_loss + loss.item()

                _, predicted = torch.max(y_pred.data, 1)

                test_accuracy =  test_accuracy  + int(torch.sum(predicted == labels.data))
            count_test = len(glob.glob(test_data_directory+'/**/*.jpg'))
            test_accuracy =test_accuracy / count_test
            test_loss =test_loss / count_test

            print(f"Epochs : {epoch+1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}")
            if(apply_wandb_log):
                wandb.log({"train_accuracy": train_accuracy, "train_loss" : train_loss, "val_accuracy": test_accuracy, "val_error": test_loss})
        epoch+=1


"""## Train"""
if __name__ == "__main__":
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # # Implemented Arg parse to take input of the hyperparameters from the command.
    parser = argparse.ArgumentParser(description="Stores all the hyperpamaters for the model.")
    parser.add_argument("-wp" , '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard' , type=str, default='Deep_Learning_A2')
    parser.add_argument("-we", "--wandb_entity",type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard." , default="cs23m013")
    parser.add_argument('-ep', '--epochs', help="Number of epochs to train neural network.", type=int, default=10)
    parser.add_argument("-nf", "--num_filters",default="3", type=int, help="Number of filters in the convolutianal neural network.")
    parser.add_argument('-lr', '--learning_rate', help = 'Learning rate used to optimize model parameters', type=float, default=0.001)
    parser.add_argument('-ac', '--activation', help='choices: ["Mish", "SiLU", "GELU", "ReLU"]', choices = ["Mish", "SiLU", "GELU", "ReLU"],type=str, default='ReLU')
    parser.add_argument("-ff", "--filter_multiplier",default="1", type=float, choices=[1, 0.5, 2])
    parser.add_argument("-df", "--dropout_factor", help="Dropout factor" , default=0.3, type=float)
    parser.add_argument("-nc","--number_of_classes",type=int,default=10)
    parser.add_argument("-ks","--kernel_size",type=int,default=3)
    parser.add_argument("-lg","--logger",type=bool,default=False,choices=[True,False] , help="Log to wandb or not" )
    parser.add_argument("-ag","--augment",default=True,help="Do Augmentation" , type=bool , choices=[True,False])
    parser.add_argument("-is","--imgSize",default=256,help="Image Size" , type=int , choices=[246 , 256])
    parser.add_argument('-trd', '--train_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/train/')
    parser.add_argument('-tsd', '--test_data_directory', help="Dataset", type=str, default='/content/drive/MyDrive/Deeplearning/inaturalist_12K/val/')


    args = parser.parse_args()

    print("Test Dataset Path:", args.test_data_directory)
    print("WandbProject:", args.wandb_project)
    print("Wandb Entity:", args.wandb_entity)
    print("Activation:", args.activation )
    print("Epochs:" ,args.epochs )
    print("Number of filter:" ,args.num_filters )
    print("Learning rate:", args.learning_rate)
    print("Dropout factor:" ,args.dropout_factor )
    print("Num factor:" , args.filter_multiplier)

    vggnet.classifier[6] = torch.nn.Linear( 4096,10)
    vggnet.classifier.add_module("7", torch.nn.LogSoftmax(dim=1))
    vggnet.to(device)

    image_size=(args.imgSize , args.imgSize)
    main(vggnet, args.learning_rate ,args.epochs, args.logger , image_size , args.augment , args.train_data_directory , args.test_data_directory)




